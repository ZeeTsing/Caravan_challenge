{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook adapted from https://www.kaggle.com/bguberfain/naive-keras\n",
    "\n",
    "Setup the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import downscale_local_mean\n",
    "from os.path import join\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img, ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'G:/Github/Caravan_challenge/data/train/'\n",
    "mask_dir = 'G:/Github/Caravan_challenge/data/train_masks/'\n",
    "test_dir = 'G:/Github/Caravan_challenge/data/test/'\n",
    "all_images = os.listdir(data_dir)\n",
    "image_size = 112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, validation_images = train_test_split(all_images, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator that we will use to read the data from the directory\n",
    "def data_gen_small(data_dir, mask_dir, images, batch_size, dims):\n",
    "        \"\"\"\n",
    "        data_dir: where the actual images are kept\n",
    "        mask_dir: where the actual masks are kept\n",
    "        images: the filenames of the images we want to generate batches from\n",
    "        batch_size: self explanatory\n",
    "        dims: the dimensions in which we want to rescale our images, tuple\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            ix = np.random.choice(np.arange(len(images)), batch_size)\n",
    "            imgs = []\n",
    "            labels = []\n",
    "            for i in ix:\n",
    "                # images\n",
    "                original_img = load_img(data_dir + images[i])\n",
    "                resized_img = original_img.resize(dims)\n",
    "                array_img = img_to_array(resized_img)/255\n",
    "                imgs.append(array_img)\n",
    "                \n",
    "                # masks\n",
    "                original_mask = load_img(mask_dir + images[i].split(\".\")[0] + '_mask.gif')\n",
    "                resized_mask = original_mask.resize(dims)\n",
    "                array_mask = img_to_array(resized_mask)/255\n",
    "                labels.append(array_mask[:, :, 0])\n",
    "            imgs = np.array(imgs)\n",
    "            labels = np.array(labels)\n",
    "            yield imgs, labels.reshape(-1, dims[0], dims[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_gen_small(data_dir, mask_dir, train_images, 5, (image_size, image_size))\n",
    "val_gen = data_gen_small(data_dir, mask_dir, validation_images, 5, (image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4070"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our own dice_coef (F1) for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use Tensorflow to write our own dice_coeficcient metric\n",
    "def dice_coef(y_true, y_pred):\n",
    "    smooth = 1e-5\n",
    "    \n",
    "    y_true = tf.round(tf.reshape(y_true, [-1]))\n",
    "    y_pred = tf.round(tf.reshape(y_pred, [-1]))\n",
    "    \n",
    "    isct = tf.reduce_sum(y_true * y_pred)\n",
    "    \n",
    "    return 2 * isct / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a simple CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "model = Sequential()\n",
    "model.add( Conv2D(8, 3, activation='relu', padding='same', input_shape=(image_size, image_size, 3)) )\n",
    "model.add( Conv2D(16, 3, activation='relu', padding='same') )\n",
    "model.add( Conv2D(1, 5, activation='sigmoid', padding='same') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'Adam', loss = BinaryCrossentropy(), metrics=['accuracy',dice_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 112, 112, 8)       224       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 112, 112, 16)      1168      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 112, 112, 1)       401       \n",
      "=================================================================\n",
      "Total params: 1,793\n",
      "Trainable params: 1,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include the epoch in the file name (uses `str.format`)\n",
    "checkpoint_path = \"G:/Github/Caravan_challenge/training_tf/naive/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, \n",
    "    verbose=1, \n",
    "    save_weights_only=True,\n",
    "    save_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 110 steps\n",
      "Epoch 1/37\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_gen, \n",
    "          epochs=37, \n",
    "          callbacks=[cp_callback],\n",
    "          steps_per_epoch=110, \n",
    "          validation_data = val_gen,  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = model.history.history['loss']\n",
    "test_loss = model.history.history['val_loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss = model.history.history['accuracy']\n",
    "test_loss = model.history.history['val_accuracy']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Accuracy', 'Val Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
