{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet_Carvana_full_fullsize.ipynb",
      "provenance": [],
      "mount_file_id": "1IlbOWmOKp8tZuUOBWYYiqJOnrp6mSgLs",
      "authorship_tag": "ABX9TyMrAzFBYJbok7lMCeEcdWuh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeeTsing/Carvana_challenge/blob/master/Unet_Carvana_full_fullsize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVTkm4oFuQLN",
        "colab_type": "text"
      },
      "source": [
        "# Setting up notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5nM5eLUpAbC",
        "colab_type": "code",
        "outputId": "ad1b1405-3fce-45cb-874c-54d6996ee67a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ-UPBRepV2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow_addons as tfa\n",
        "import PIL\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Input, MaxPool2D, UpSampling2D, Concatenate, Conv2DTranspose\n",
        "from tensorflow.keras import Sequential,Model\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img, ImageDataGenerator\n",
        "import os\n",
        "from tensorflow.keras.backend import flatten\n",
        "import tensorflow.keras.backend as K\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vXZiP9tuVaM",
        "colab_type": "text"
      },
      "source": [
        "# Prepare data generator (no augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRnv2O_upYZ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/drive/My Drive/car_data/train/'\n",
        "mask_dir = '/content/drive/My Drive/car_data/train_masks/'\n",
        "\n",
        "all_images = os.listdir(data_dir)\n",
        "\n",
        "to_train = 1 # ratio of number of train set images to use\n",
        "total_train_images = all_images[:int(len(all_images)*to_train)]\n",
        "\n",
        "WIDTH = 512  #  actual : 1918//1920 divisive by 64\n",
        "HEIGHT = 512 # actual : 1280\n",
        "BATCH_SIZE = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2quls8NOptdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split train set and test set\n",
        "train_images, validation_images = train_test_split(total_train_images, train_size=0.8, test_size=0.2,random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmS31FAbI-YR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generator that we will use to read the data from the directory\n",
        "def data_gen_small(data_dir, mask_dir, images, batch_size, dims):\n",
        "        \"\"\"\n",
        "        data_dir: where the actual images are kept\n",
        "        mask_dir: where the actual masks are kept\n",
        "        images: the filenames of the images we want to generate batches from\n",
        "        batch_size: self explanatory\n",
        "        dims: the dimensions in which we want to rescale our images, tuple\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ix = np.random.choice(np.arange(len(images)), batch_size)\n",
        "            imgs = []\n",
        "            labels = []\n",
        "            for i in ix:\n",
        "                # images\n",
        "                original_img = load_img(data_dir + images[i])\n",
        "                resized_img = original_img.resize(dims)\n",
        "                array_img = img_to_array(resized_img)/255\n",
        "                imgs.append(array_img)\n",
        "                \n",
        "                # masks\n",
        "                original_mask = load_img(mask_dir + images[i].split(\".\")[0] + '_mask.gif')\n",
        "                resized_mask = original_mask.resize(dims)\n",
        "                array_mask = img_to_array(resized_mask)/255\n",
        "                labels.append(array_mask[:, :, 0])\n",
        "\n",
        "            imgs = np.array(imgs)\n",
        "            labels = np.array(labels)\n",
        "            yield imgs, labels.reshape(-1, dims[0], dims[1], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoBJOAfFj76S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generator that we will use to read the data from the directory with random augmentation\n",
        "def data_gen_aug(data_dir, mask_dir, images, batch_size, dims):\n",
        "        \"\"\"\n",
        "        data_dir: where the actual images are kept\n",
        "        mask_dir: where the actual masks are kept\n",
        "        images: the filenames of the images we want to generate batches from\n",
        "        batch_size: self explanatory\n",
        "        dims: the dimensions in which we want to rescale our images, tuple\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            ix = np.random.choice(np.arange(len(images)), batch_size)\n",
        "            imgs = []\n",
        "            labels = []\n",
        "            for i in ix:\n",
        "                # read images and masks\n",
        "                original_img = load_img(data_dir + images[i])\n",
        "                original_mask = load_img(mask_dir + images[i].split(\".\")[0] + '_mask.gif')\n",
        "                \n",
        "                # transform into ideal sizes\n",
        "                resized_img = original_img.resize(dims)\n",
        "                resized_mask = original_mask.resize(dims)\n",
        "              \n",
        "                # add random augmentation > here we only flip horizontally\n",
        "                if np.random.random() < 0.5:\n",
        "                  resized_img = resized_img.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "                  resized_mask = resized_mask.transpose(PIL.Image.FLIP_LEFT_RIGHT)\n",
        "\n",
        "                array_img = img_to_array(resized_img)/255\n",
        "                array_mask = img_to_array(resized_mask)/255\n",
        "\n",
        "                imgs.append(array_img)\n",
        "                labels.append(array_mask[:, :, 0])\n",
        "                \n",
        "            imgs = np.array(imgs)\n",
        "            labels = np.array(labels)\n",
        "            yield imgs, labels.reshape(-1, dims[0], dims[1], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baC_mlppp5LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generator for train and validation data set\n",
        "train_gen = data_gen_aug(data_dir, mask_dir, train_images, BATCH_SIZE, (WIDTH, HEIGHT))\n",
        "val_gen = data_gen_small(data_dir, mask_dir, validation_images, BATCH_SIZE, (WIDTH, HEIGHT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEDhS9TvuHid",
        "colab_type": "text"
      },
      "source": [
        "# Set up Unet model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdLWK4FHubuF",
        "colab_type": "text"
      },
      "source": [
        "Define down and up layers that will be used in Unet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eD3PryxOCACg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def down(input_layer, filters, pool=True):\n",
        "    conv1 = Conv2D(filters, (3, 3), padding='same', activation='relu')(input_layer)\n",
        "    residual = Conv2D(filters, (3, 3), padding='same', activation='relu')(conv1)\n",
        "    if pool:\n",
        "        max_pool = MaxPool2D()(residual)\n",
        "        return max_pool, residual\n",
        "    else:\n",
        "        return residual\n",
        "\n",
        "def up(input_layer, residual, filters):\n",
        "    filters=int(filters)\n",
        "    upsample = UpSampling2D()(input_layer)\n",
        "    upconv = Conv2D(filters, kernel_size=(2, 2), padding=\"same\")(upsample)\n",
        "    concat = Concatenate(axis=3)([residual, upconv])\n",
        "    conv1 = Conv2D(filters, (3, 3), padding='same', activation='relu')(concat)\n",
        "    conv2 = Conv2D(filters, (3, 3), padding='same', activation='relu')(conv1)\n",
        "    return conv2  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx3z9JdAugdW",
        "colab_type": "code",
        "outputId": "8188dd33-8661-4e53-815d-c85e0333a0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Make a custom U-nets implementation.\n",
        "filters = 64\n",
        "input_layer = Input(shape = [WIDTH, HEIGHT, 3])\n",
        "layers = [input_layer]\n",
        "residuals = []\n",
        "\n",
        "# Down 1\n",
        "d1, res1 = down(input_layer, filters)\n",
        "residuals.append(res1)\n",
        "\n",
        "filters *= 2\n",
        "\n",
        "# Down 2\n",
        "d2, res2 = down(d1, filters)\n",
        "residuals.append(res2)\n",
        "\n",
        "filters *= 2\n",
        "\n",
        "# Down 3\n",
        "d3, res3 = down(d2, filters)\n",
        "residuals.append(res3)\n",
        "\n",
        "filters *= 2\n",
        "\n",
        "# Down 4\n",
        "d4, res4 = down(d3, filters)\n",
        "residuals.append(res4)\n",
        "\n",
        "filters *= 2\n",
        "\n",
        "# Down 5\n",
        "d5 = down(d4, filters, pool=False)\n",
        "\n",
        "# Up 1\n",
        "up1 = up(d5, residual=residuals[-1], filters=filters/2)\n",
        "filters /= 2\n",
        "\n",
        "# Up 2\n",
        "up2 = up(up1, residual=residuals[-2], filters=filters/2)\n",
        "\n",
        "filters /= 2\n",
        "\n",
        "# Up 3\n",
        "up3 = up(up2, residual=residuals[-3], filters=filters/2)\n",
        "\n",
        "filters /= 2\n",
        "\n",
        "# Up 4\n",
        "up4 = up(up3, residual=residuals[-4], filters=filters/2)\n",
        "\n",
        "out = Conv2D(filters=1, kernel_size=(1, 1), activation=\"sigmoid\")(up4)\n",
        "\n",
        "model = Model(input_layer, out)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 512, 512, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 512, 512, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 512, 512, 64) 36928       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 256, 256, 64) 0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 256, 256, 128 73856       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 256, 256, 128 147584      conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 128, 128, 128 0           conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 128, 128, 256 295168      max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 128, 128, 256 590080      conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 256)  0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 64, 512)  1180160     max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 64, 64, 512)  2359808     conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 512)  0           conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 1024) 4719616     max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 1024) 9438208     conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d (UpSampling2D)    (None, 64, 64, 1024) 0           conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 64, 64, 512)  2097664     up_sampling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 64, 64, 1024) 0           conv2d_7[0][0]                   \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 64, 64, 512)  4719104     concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 64, 64, 512)  2359808     conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_1 (UpSampling2D)  (None, 128, 128, 512 0           conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 128, 128, 256 524544      up_sampling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 128, 128, 512 0           conv2d_5[0][0]                   \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 128, 128, 256 1179904     concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 128, 128, 256 590080      conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_2 (UpSampling2D)  (None, 256, 256, 256 0           conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 256, 256, 128 131200      up_sampling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 256, 256, 256 0           conv2d_3[0][0]                   \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 256, 256, 128 295040      concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 256, 256, 128 147584      conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "up_sampling2d_3 (UpSampling2D)  (None, 512, 512, 128 0           conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 512, 512, 64) 32832       up_sampling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 512, 512, 128 0           conv2d_1[0][0]                   \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 512, 512, 64) 73792       concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 512, 512, 64) 36928       conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 512, 512, 1)  65          conv2d_21[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 31,031,745\n",
            "Trainable params: 31,031,745\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yiF0byj0DNId",
        "colab": {}
      },
      "source": [
        "# Now let's use Tensorflow to write our own dice_coeficcient metric, which is a effective indicator of how much two sets overlap with each other\n",
        "def dice_coef(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = flatten(y_true)\n",
        "    y_pred_f = flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "\n",
        "#the loss function below tries to combine both binary cross entropy with dice loss to try to minimize micro loss but also global loss\n",
        "#ref: https://towardsdatascience.com/understanding-dice-loss-for-crisp-boundary-detection-bb30c2e5f62b\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    return BinaryCrossentropy(y_true, y_pred) + 1 - dice_coef(y_true, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3Sd2VKBvIIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"/content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights every epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    save_freq='epoch')\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8,\n",
        "                                              restore_best_weights=False\n",
        "                                              )\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
        "                                   factor=0.2,\n",
        "                                   patience=5,\n",
        "                                   verbose=1,\n",
        "                                   min_delta=1e-4,min_lr = 1e-6\n",
        "                                   )\n",
        "\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFGgzoghvQvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = adam, loss = BinaryCrossentropy(), metrics=['accuracy',dice_coef])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxjcKy78vWBW",
        "colab_type": "code",
        "outputId": "548f8aa8-e54d-4632-ae39-f639ee364769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "model.fit(train_gen, callbacks=[cp_callback,early_stop,reduce_lr],\n",
        "                    steps_per_epoch=np.ceil(float(len(train_images)) / float(BATCH_SIZE)),\n",
        "                    epochs=100,\n",
        "                    validation_steps=np.ceil(float(len(validation_images)) / float(BATCH_SIZE)),\n",
        "                    validation_data = val_gen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 814.0 steps, validate for 204.0 steps\n",
            "Epoch 1/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0857 - accuracy: 0.9615 - dice_coef: 0.8756\n",
            "Epoch 00001: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0001.ckpt\n",
            "814/814 [==============================] - 1259s 2s/step - loss: 0.0856 - accuracy: 0.9616 - dice_coef: 0.8757 - val_loss: 0.0199 - val_accuracy: 0.9892 - val_dice_coef: 0.9714\n",
            "Epoch 2/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0516 - accuracy: 0.9776 - dice_coef: 0.9312\n",
            "Epoch 00002: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0002.ckpt\n",
            "814/814 [==============================] - 1236s 2s/step - loss: 0.0517 - accuracy: 0.9776 - dice_coef: 0.9311 - val_loss: 0.0597 - val_accuracy: 0.9736 - val_dice_coef: 0.9054\n",
            "Epoch 3/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0280 - accuracy: 0.9859 - dice_coef: 0.9606\n",
            "Epoch 00003: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0003.ckpt\n",
            "814/814 [==============================] - 1225s 2s/step - loss: 0.0280 - accuracy: 0.9859 - dice_coef: 0.9606 - val_loss: 0.0163 - val_accuracy: 0.9905 - val_dice_coef: 0.9778\n",
            "Epoch 4/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0167 - accuracy: 0.9903 - dice_coef: 0.9767\n",
            "Epoch 00004: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0004.ckpt\n",
            "814/814 [==============================] - 1225s 2s/step - loss: 0.0167 - accuracy: 0.9903 - dice_coef: 0.9767 - val_loss: 0.0160 - val_accuracy: 0.9905 - val_dice_coef: 0.9774\n",
            "Epoch 5/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0120 - accuracy: 0.9919 - dice_coef: 0.9831\n",
            "Epoch 00005: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0005.ckpt\n",
            "814/814 [==============================] - 1228s 2s/step - loss: 0.0120 - accuracy: 0.9919 - dice_coef: 0.9831 - val_loss: 0.0114 - val_accuracy: 0.9922 - val_dice_coef: 0.9851\n",
            "Epoch 6/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0098 - accuracy: 0.9927 - dice_coef: 0.9862\n",
            "Epoch 00006: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0006.ckpt\n",
            "814/814 [==============================] - 1231s 2s/step - loss: 0.0098 - accuracy: 0.9927 - dice_coef: 0.9862 - val_loss: 0.0096 - val_accuracy: 0.9928 - val_dice_coef: 0.9861\n",
            "Epoch 7/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0166 - accuracy: 0.9903 - dice_coef: 0.9771\n",
            "Epoch 00007: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0007.ckpt\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "814/814 [==============================] - 1228s 2s/step - loss: 0.0166 - accuracy: 0.9903 - dice_coef: 0.9771 - val_loss: 0.0100 - val_accuracy: 0.9927 - val_dice_coef: 0.9864\n",
            "Epoch 8/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0086 - accuracy: 0.9931 - dice_coef: 0.9877\n",
            "Epoch 00008: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0008.ckpt\n",
            "814/814 [==============================] - 1229s 2s/step - loss: 0.0086 - accuracy: 0.9931 - dice_coef: 0.9877 - val_loss: 0.0087 - val_accuracy: 0.9931 - val_dice_coef: 0.9880\n",
            "Epoch 9/100\n",
            "813/814 [============================>.] - ETA: 1s - loss: 0.0082 - accuracy: 0.9932 - dice_coef: 0.9885\n",
            "Epoch 00009: saving model to /content/drive/My Drive/car_data/unet_cp_full_512batch5/cp-0009.ckpt\n",
            "814/814 [==============================] - 1231s 2s/step - loss: 0.0082 - accuracy: 0.9932 - dice_coef: 0.9885 - val_loss: 0.0086 - val_accuracy: 0.9931 - val_dice_coef: 0.9883\n",
            "Epoch 10/100\n",
            "171/814 [=====>........................] - ETA: 14:34 - loss: 0.0077 - accuracy: 0.9934 - dice_coef: 0.9891"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmRW4-OewdBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_diagnostic_plot(model,name):\n",
        "  training_loss = model.history.history[name]\n",
        "  test_loss = model.history.history[f'val_{name}']\n",
        "\n",
        "  # Create count of the number of epochs\n",
        "  epoch_count = range(1, len(training_loss) + 1)\n",
        "\n",
        "  # Visualize loss history\n",
        "  plt.plot(epoch_count, training_loss, 'r--')\n",
        "  plt.plot(epoch_count, test_loss, 'b-')\n",
        "  plt.legend([f'Training {name}', f'Val {name}'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eH-Gy6oFK9tB",
        "colab_type": "code",
        "outputId": "a7c1dbf7-f413-4e98-efb5-7c498cb0a0cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "get_diagnostic_plot(model,'loss')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bf4702c5f680>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_diagnostic_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVNF9cq7LFnY",
        "colab_type": "code",
        "outputId": "a2f04870-e647-4db1-9aad-341f2e307ecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "get_diagnostic_plot(model,'accuracy')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c44fe2083476>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_diagnostic_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9CltyBpM7eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "get_diagnostic_plot(model,'dice_coef')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtz8dYM5NEH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "980f2215-cb5f-4854-9aa6-c9b78ad6a707"
      },
      "source": [
        "save_path = '/content/drive/My Drive/car_data/model_unet_full_512batch5/'\n",
        "tf.keras.models.save_model(model,save_path)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2c74e1716616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/car_data/model_unet_full_512batch5/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wsRyDgDp_xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}